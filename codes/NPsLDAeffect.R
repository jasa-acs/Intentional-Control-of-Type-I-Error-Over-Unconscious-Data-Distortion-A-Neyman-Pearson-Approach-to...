# The code produces the topics and the corresponding coefficients generated by NP-sLDA
load('dataGDQ1.RData')
load('count.RData')

library(nproc)
library(topicmodels)
library("e1071")
library(dsda)
library(glmnet)
library(randomForest)


set.seed(2018)
# Get the dimensions
n_1  = dim(A)[1]
n_2  = dim(B)[1]

p   = dim(A)[2]

# Generate labels
y_1 = rep(0,n_1)
y_2 = rep(1,n_2)



repl = 2
k = 10

noword = 20
wordseq1=matrix(0,noword,repl)
wordseq2=matrix(0,noword,repl)
wordseq3=matrix(0,noword,repl)
wordseq4=matrix(0,noword,repl)
wordseq5=matrix(0,noword,repl)
wordseq6=matrix(0,noword,repl)
wordseq7=matrix(0,noword,repl)
wordseq8=matrix(0,noword,repl)
wordseq9=matrix(0,noword,repl)
wordseq10=matrix(0,noword,repl)

coefficients = array(0,c(2,7,11))

for (i in 1:repl){
 
  
  print(i)
  A_shuffle = A[sample(nrow(A)),]
  B_shuffle = B[sample(nrow(B)),]
  
  r = 2
  
  A_train   = A_shuffle[1:floor(n_1/r),]
  B_train   = B_shuffle[1:floor(n_2/r),]
  data1     = rbind(A_train,B_train)
  object = LDA(data1, k, method = "Gibbs", control= NULL, model = NULL)
  
  wordprob = object@beta
  topic1   = sort(wordprob[1,], decreasing=TRUE, index.return=TRUE)$ix
  topic2   = sort(wordprob[2,], decreasing=TRUE, index.return=TRUE)$ix
  topic3   = sort(wordprob[3,], decreasing=TRUE, index.return=TRUE)$ix
  topic4   = sort(wordprob[4,], decreasing=TRUE, index.return=TRUE)$ix
  topic5   = sort(wordprob[5,], decreasing=TRUE, index.return=TRUE)$ix
  topic6   = sort(wordprob[6,], decreasing=TRUE, index.return=TRUE)$ix
  topic7   = sort(wordprob[7,], decreasing=TRUE, index.return=TRUE)$ix
  topic8   = sort(wordprob[8,], decreasing=TRUE, index.return=TRUE)$ix
  topic9   = sort(wordprob[9,], decreasing=TRUE, index.return=TRUE)$ix
  topic10   = sort(wordprob[10,], decreasing=TRUE, index.return=TRUE)$ix
  
  
  
  wordseq1[,i] = as.vector(count[topic1[1:noword],])
  wordseq2[,i] = as.vector(count[topic2[1:noword],])
  wordseq3[,i] = as.vector(count[topic3[1:noword],])
  wordseq4[,i] = as.vector(count[topic4[1:noword],])
  wordseq5[,i] = as.vector(count[topic5[1:noword],])
  wordseq6[,i] = as.vector(count[topic6[1:noword],])
  wordseq7[,i] = as.vector(count[topic7[1:noword],])
  wordseq8[,i] = as.vector(count[topic8[1:noword],])
  wordseq9[,i] = as.vector(count[topic9[1:noword],])
  wordseq10[,i] = as.vector(count[topic10[1:noword],])

  X_train = object@gamma
  y_train = c(y_1[1:floor(n_1/r)],y_2[1:floor(n_2/r)])
  
  
  
  fitslda = npc(X_train, y_train, split= 7, method = "slda", delta= 0.3,alpha = 0.2, split.ratio = "adaptive")
  print(coef( fitslda$fit[[1]]$fit))
  print(coef( fitslda$fit[[2]]$fit))
  print(coef( fitslda$fit[[3]]$fit))
  print(coef( fitslda$fit[[4]]$fit))
  print(coef( fitslda$fit[[5]]$fit))
  print(coef( fitslda$fit[[6]]$fit))
  print(coef( fitslda$fit[[7]]$fit))
  coefficients[i,1,] = as.vector(coef( fitslda$fit[[1]]$fit))
  coefficients[i,2,] = as.vector(coef( fitslda$fit[[2]]$fit))
  coefficients[i,3,] = as.vector(coef( fitslda$fit[[3]]$fit))
  coefficients[i,4,] = as.vector(coef( fitslda$fit[[4]]$fit))
  coefficients[i,5,] = as.vector(coef( fitslda$fit[[5]]$fit))
  coefficients[i,6,] = as.vector(coef( fitslda$fit[[6]]$fit))
  coefficients[i,7,] = as.vector(coef( fitslda$fit[[7]]$fit))
  
}


save(wordseq1,wordseq2,wordseq3,wordseq4,wordseq5,wordseq6,wordseq7,wordseq8,wordseq9,wordseq10,file="NPsLDAeffect-word.RData")
save(coefficients, file = "NPsLDAeffect-coefficient.RData")

print(cat("topic 1", wordseq1[,i]))
print(cat("topic 2", wordseq2[,i]))
print(cat("topic 3",wordseq3[,i]))
print(cat("topic 4",wordseq4[,i]))
print(cat("topic 5",wordseq5[,i]))
print(cat("topic 6", wordseq6[,i]))
print(cat("topic 7", wordseq7[,i]))
print(cat("topic 8",wordseq8[,i]))
print(cat("topic 9",wordseq9[,i]))
print(cat("topic 10",wordseq10[,i]))






